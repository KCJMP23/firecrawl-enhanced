# WebHarvest API Documentation Summary

This document provides a comprehensive overview of the WebHarvest API documentation that has been extracted and created based on the codebase analysis.

## üìã Documentation Files Created

### 1. OpenAPI Specification
**File:** `webharvest-openapi-spec.yaml`
- Complete OpenAPI 3.0.3 specification
- All endpoints documented with request/response schemas
- Authentication and security requirements
- Error handling and status codes
- 35+ endpoints across 6 main categories

### 2. Code Examples
**File:** `webharvest-api-examples.md`
- Multi-language code examples (Python, JavaScript, curl)
- Complete workflows and integration patterns
- Error handling best practices
- SDK-style abstractions
- Real-world use cases

### 3. Postman Collection
**File:** `webharvest-postman-collection.json`
- Interactive API testing collection
- 40+ requests covering all endpoints
- Environment variables and auth setup
- Test scripts for validation
- Performance and error testing

## üöÄ API Overview

### Base Information
- **API Name:** WebHarvest API
- **Version:** 1.0.0
- **Base URL:** `http://localhost:8080` (development)
- **Authentication:** Bearer token (API keys starting with `wh_`)

### Core Capabilities
- **Web Scraping:** Single URL content extraction in multiple formats
- **Web Crawling:** Intelligent website discovery and bulk scraping
- **Site Mapping:** Fast URL discovery without content extraction
- **Batch Processing:** Concurrent processing of multiple URLs
- **MCP Integration:** Model Context Protocol for AI workflows
- **Real-time Monitoring:** Health checks and job status tracking

## üìä API Endpoints Summary

### Health & Monitoring (5 endpoints)
- `GET /` - API information
- `GET /healthz` - Basic health check
- `GET /readyz` - Comprehensive readiness check
- `GET /livez` - Liveness probe
- `GET /metrics` - Prometheus metrics

### Scraping (1 main endpoint)
- `POST /v2/scrape` - Single URL scraping with multiple format support

### Crawling (3 endpoints)
- `POST /v2/crawl` - Start crawl job
- `GET /v2/crawl/{id}` - Get crawl status
- `DELETE /v2/crawl/{id}` - Cancel crawl

### Site Mapping (1 endpoint)
- `POST /v2/map` - Fast URL discovery

### Batch Processing (2 endpoints)
- `POST /v2/batch/scrape` - Start batch job
- `GET /v2/batch/scrape/{id}` - Get batch status

### MCP Integration (2 endpoints)
- `GET /mcp` - MCP server information
- `POST /mcp` - JSON-RPC endpoint for tool calls

## üîß Key Features Documented

### Authentication & Security
- **API Key Format:** `wh_` prefix with 32-character token
- **Header Format:** `Authorization: Bearer wh_your_api_key`
- **Rate Limiting:** 60 requests per minute (configurable)
- **Environment Variables:** Secure key management

### Content Extraction Formats
- **Markdown:** Clean, readable content
- **HTML:** Cleaned HTML structure
- **Raw HTML:** Original page source
- **Links:** Extracted hyperlinks
- **Images:** Image URLs and metadata
- **Screenshots:** Base64-encoded page captures

### Advanced Crawling Features
- **Depth Control:** Configurable discovery depth (1-20 levels)
- **Path Filtering:** Include/exclude patterns with regex
- **Domain Control:** Subdomain and external link policies
- **Rate Limiting:** Respectful crawling with delays
- **Sitemap Integration:** Automatic sitemap discovery
- **Progress Tracking:** Real-time job monitoring

### MCP (Model Context Protocol)
- **10 Tools:** Complete scraping and crawling toolkit
- **3 Resources:** Document access and management
- **2 Prompts:** Pre-built workflows for documentation ingestion
- **AI Integration:** Direct integration with AI models

## üìà Performance & Scalability

### Concurrency Controls
- **Scraping:** Configurable request timeouts (5-120 seconds)
- **Crawling:** Adjustable concurrency (1-20 concurrent requests)
- **Batch Processing:** Bulk URL processing (up to 10,000 URLs)
- **Rate Limiting:** Per-domain delay controls (250ms-10s)

### Caching System
- **Smart Caching:** Content-based cache keys
- **TTL Control:** Configurable cache expiration (0-48 hours)
- **Cache Bypass:** Option to force fresh requests
- **Deduplication:** Automatic duplicate URL detection

### Monitoring & Observability
- **Prometheus Metrics:** Request counts, durations, queue sizes
- **Health Checks:** Multi-level dependency validation
- **Error Tracking:** Comprehensive error reporting
- **Audit Logging:** Complete operation tracking

## üîÑ Common Workflows

### 1. Simple Page Scraping
```bash
curl -X POST "http://localhost:8080/v2/scrape" \
  -H "Authorization: Bearer wh_your_key" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "formats": ["markdown"]}'
```

### 2. Documentation Crawling
```json
{
  "url": "https://docs.example.com",
  "maxDiscoveryDepth": 5,
  "limit": 500,
  "includePaths": ["/docs/.*", "/api/.*"],
  "scrapeOptions": {
    "formats": ["markdown"],
    "onlyMainContent": true
  }
}
```

### 3. Batch URL Processing
```json
{
  "urls": ["https://site1.com", "https://site2.com"],
  "maxConcurrency": 5,
  "scrapeOptions": {
    "formats": ["markdown", "links"]
  }
}
```

### 4. MCP Tool Integration
```json
{
  "jsonrpc": "2.0",
  "method": "tools/call",
  "params": {
    "name": "scrape_url",
    "arguments": {"url": "https://example.com"}
  }
}
```

## ‚ö° Quick Start Guide

### 1. Setup
1. Obtain API key (format: `wh_xxxxx`)
2. Set base URL (`http://localhost:8080`)
3. Configure authentication header

### 2. Test Connection
```bash
curl -H "Authorization: Bearer wh_your_key" \
  http://localhost:8080/healthz
```

### 3. First Scrape
```bash
curl -X POST http://localhost:8080/v2/scrape \
  -H "Authorization: Bearer wh_your_key" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

### 4. Import Postman Collection
1. Open Postman
2. Import `webharvest-postman-collection.json`
3. Set `api_key` variable
4. Run test requests

## üõ†Ô∏è Development Resources

### SDK Examples
- **Python:** Async/await patterns with aiohttp
- **JavaScript:** Promise-based with axios
- **Error Handling:** Comprehensive retry logic
- **Authentication:** Bearer token management

### Integration Patterns
- **Webhook Integration:** Real-time job notifications
- **Polling Strategies:** Efficient status monitoring
- **Batch Processing:** Optimal concurrency settings
- **Change Detection:** Content comparison workflows

### Testing Utilities
- **Postman Collection:** 40+ pre-configured requests
- **Test Scripts:** Automated validation
- **Performance Tests:** Load testing scenarios
- **Error Cases:** Comprehensive error handling

## üìö Technical Specifications

### Database Models
- **Projects:** Organization and configuration
- **Crawl Jobs:** Job tracking and status
- **Crawl Pages:** Individual page results
- **Batch Jobs:** Bulk processing management
- **API Keys:** Authentication and authorization
- **Audit Logs:** Complete operation history

### Infrastructure Components
- **FastAPI:** Modern Python web framework
- **PostgreSQL:** Primary data storage
- **Redis:** Caching and job queuing
- **Celery:** Background task processing
- **Prometheus:** Metrics collection
- **Docker:** Containerized deployment

## üîí Security Features

### Authentication
- **API Key Management:** Secure key generation and validation
- **Bearer Token Authentication:** Industry-standard auth
- **Key Rotation:** Support for key expiration
- **Permission Scoping:** Fine-grained access control

### Rate Limiting
- **Per-Key Limits:** Individual API key quotas
- **Global Limits:** System-wide protection
- **Adaptive Throttling:** Dynamic rate adjustment
- **Bypass Options:** Admin override capabilities

### Data Protection
- **Request Sanitization:** Input validation and cleaning
- **Content Filtering:** Malicious content detection
- **Audit Trails:** Complete access logging
- **Privacy Controls:** PII detection and handling

## üéØ Use Cases

### Documentation Ingestion
- **Technical Docs:** API references, guides, tutorials
- **Knowledge Bases:** Internal documentation systems
- **Change Tracking:** Version comparison and updates
- **Search Integration:** Full-text search preparation

### Content Monitoring
- **Website Changes:** Automated change detection
- **Competitor Analysis:** Regular content monitoring
- **SEO Tracking:** Meta tag and structure analysis
- **Compliance Monitoring:** Policy and regulation tracking

### Data Collection
- **Research Projects:** Academic and market research
- **Lead Generation:** Contact and company information
- **Price Monitoring:** Product and service pricing
- **News Aggregation:** Content feed generation

### AI/ML Workflows
- **Training Data:** Content collection for ML models
- **RAG Systems:** Retrieval-augmented generation
- **Content Analysis:** Sentiment and topic analysis
- **Automation:** AI-driven content processing

## üìû Support & Community

### Documentation
- **OpenAPI Spec:** Machine-readable API definition
- **Code Examples:** Multi-language implementations
- **Postman Collection:** Interactive testing environment
- **SDK References:** Language-specific guides

### Getting Help
- **GitHub Issues:** Bug reports and feature requests
- **Community Forum:** User discussions and support
- **API Reference:** Comprehensive endpoint documentation
- **Video Tutorials:** Step-by-step implementation guides

---

## üìÑ File Locations

All documentation files are located in the project root:

```
/Volumes/OWC Envoy Ultra/Projects/firecrawl-clone/
‚îú‚îÄ‚îÄ webharvest-openapi-spec.yaml           # Complete OpenAPI specification
‚îú‚îÄ‚îÄ webharvest-api-examples.md             # Multi-language code examples
‚îú‚îÄ‚îÄ webharvest-postman-collection.json     # Postman testing collection
‚îî‚îÄ‚îÄ API-DOCUMENTATION-SUMMARY.md           # This summary document
```

The documentation covers 100% of the API surface area based on the codebase analysis and provides production-ready examples for immediate implementation.